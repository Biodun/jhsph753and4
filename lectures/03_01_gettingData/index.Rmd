---
title       : Getting data
subtitle    : 
author      : Jeffrey Leek
job         : Johns Hopkins Bloomberg School of Public Health
logo        : bloomberg_shield.png
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
url:
  lib: ../../libraries
  assets: ../../assets
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/',cache=TRUE)

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## Pro Tip

__Once you know something there is temptation to be a snob. Don't be.__

<img class="center" src="../../assets/img/purity.png" height=400>


http://simplystatistics.org/2014/01/28/marie-curie-says-stop-hating-on-quilt-plots-already/

---

## Paper of the day

Statistical inference for exploratory data analysis and model diagnostics

"We propose to furnish visual statistical methods with an inferential framework and protocol, modelled on confirmatory statistical testing. In this framework, plots take on the role of test statistics, and human cognition the role of statistical tests."

http://rsta.royalsocietypublishing.org/content/367/1906/4361.full


---

## My belief about statisticians

* Define the question
* Define the ideal data set
* <redtext>Determine what data you can access </redtext>
* <redtext>Obtain the data </redtext>
* <redtext>Clean the data </redtext>
* Exploratory data analysis
* Statistical prediction/modeling
* Interpret results
* Challenge results
* Synthesize/write up results
* Create reproducible code


---


## Why? 

<center> "Real scientists make their own data" -Sean Taylor</center>

<center> "Real scientists make (and know how to get) their own data" -Jeff </center>

1. Most major contributions to real science involve data collection (by you/others)
2. Making/getting your own data gives you priveledged access to scientific findings
3. If you create the data you will understand it better
4. You can do things like experimental design (e.g. randomized experiments)

[http://seanjtaylor.com/post/41463778912/real-scientists-make-their-own-data](http://seanjtaylor.com/post/41463778912/real-scientists-make-their-own-data)


---

## What you wish data looked like

<img class=center src=../../assets/img/03_ObtainingData/excel.png height=450>


---

## What does data really look like? 

<img class=center src=../../assets/img/03_ObtainingData/fastq.png height=450/>


[http://brianknaus.com/software/srtoolbox/s_4_1_sequence80.txt](http://brianknaus.com/software/srtoolbox/s_4_1_sequence80.txt)


--- 

## What does data really look like? 

<img class=center src=../../assets/img/03_ObtainingData/twitter.png height= 450/>


[https://dev.twitter.com/docs/api/1/get/blocks/blocking](https://dev.twitter.com/docs/api/1/get/blocks/blocking)

---

## What does data really look like?


<img class=center src=../../assets/img/03_ObtainingData/medicalrecord.png height=400/>


[http://blue-button.github.com/challenge/](http://blue-button.github.com/challenge/)



---

## What does data really look like?

<img class=center src=../../assets/img/fmri.jpeg height= 450/>


[http://spectrum.ieee.org/tech-talk/biomedical/imaging/sorting-fmri-with-textmining-software](http://spectrum.ieee.org/tech-talk/biomedical/imaging/sorting-fmri-with-textmining-software)

---

## Where is data?


<img class=center src=../../assets/img/harddrive.jpg height= 450/>

[http://en.wikipedia.org/wiki/Sneakernet](http://en.wikipedia.org/wiki/Sneakernet)

[http://en.wikipedia.org/wiki/File:USB-Hard-Drive.jpg](http://en.wikipedia.org/wiki/File:USB-Hard-Drive.jpg)

---

## Where is data?

<img class=center src=../../assets/img/03_ObtainingData/databases.png height=400/>


[http://rickosborne.org/blog/2010/02/infographic-migrating-from-sql-to-mapreduce-with-mongodb/](http://rickosborne.org/blog/2010/02/infographic-migrating-from-sql-to-mapreduce-with-mongodb/)


---

## Where is data?

<img class=center src=../../assets/img/03_ObtainingData/twitter.png height= 450/>

[https://dev.twitter.com/docs/api/1/get/blocks/blocking](https://dev.twitter.com/docs/api/1/get/blocks/blocking)


---

## Where is data?

<img class=center src=../../assets/img/03_ObtainingData/baltimore.png height= 450/>

[https://data.baltimorecity.gov/](https://data.baltimorecity.gov/)

---

## Definition of data

<q>Data are values of qualitative or quantitative variables, belonging to a set of items.</q>

[http://en.wikipedia.org/wiki/Data](http://en.wikipedia.org/wiki/Data)

---

## Definition of data
<q>Data are values of qualitative or quantitative variables, belonging to a <redtext>set of items</redtext>.</q>

[http://en.wikipedia.org/wiki/Data](http://en.wikipedia.org/wiki/Data)

__Set of items__: Sometimes called the population; the set of objects you are interested in

---

## Definition of data
<q>Data are values of qualitative or quantitative <redtext>variables</redtext>, belonging to a set of items.</q>

[http://en.wikipedia.org/wiki/Data](http://en.wikipedia.org/wiki/Data)

__Variables__: A measurement or characteristic of an item.


---

## Definition of data
<q>Data are values of <redtext>qualitative</redtext> or <redtext>quantitative</redtext> variables, belonging to a set of items.</q>

[http://en.wikipedia.org/wiki/Data](http://en.wikipedia.org/wiki/Data)


__Qualitative__: Country of origin, sex, treatment

__Quantitative__: Height, weight, blood pressure

---

## Raw versus processed data

__Raw data__
* The original source of the data
* Often hard to use for data analyses
* Data analysis _includes_ processing
* Raw data may only need to be processed once

[http://en.wikipedia.org/wiki/Raw_data](http://en.wikipedia.org/wiki/Raw_data)

__Processed data__
* Data that is ready for analysis
* Processing can include merging, subsetting, transforming, etc.
* There may be standards for processing
* All steps should be recorded 

[http://en.wikipedia.org/wiki/Computer_data_processing](http://en.wikipedia.org/wiki/Computer_data_processing)

---

## An example of a processing pipeline

<img class=center src=../../assets/img/03_ObtainingData/hiseq.jpeg height=450/>

[http://www.illumina.com.cn/support/sequencing/sequencing_instruments/hiseq_1000.asp](http://www.illumina.com.cn/support/sequencing/sequencing_instruments/hiseq_1000.asp)

---

## An example of a processing pipeline

<img class=center src=../../assets/img/03_ObtainingData/processing.png height=400 />

[http://www.cbcb.umd.edu/~hcorrada/CMSC858B/lectures/lect22_seqIntro/seqIntro.pdf](http://www.cbcb.umd.edu/~hcorrada/CMSC858B/lectures/lect22_seqIntro/seqIntro.pdf)

---

---

## The raw data


* The strange binary file your measurement machine spits out
* The unformatted Excel file with 10 worksheets the company you contracted with sent you
* The complicated JSON data you got from scraping the Twitter API
* The hand-entered numbers you collected looking through a microscope

_You know the raw data is in the right format if you_ 

1. Ran no software on the data
2. Did not manipulate any of the numbers in the data
3. You did not remove any data from the data set
4. You did not summarize the data in any way

[https://github.com/jtleek/datasharing](https://github.com/jtleek/datasharing)


---

## The tidy data

1. Each variable you measure should be in one column
2. Each different observation of that variable should be in a different row
3. There should be one table for each "kind" of variable
4. If you have multiple tables, they should include a column in the table that allows them to be linked

_Some other important tips_

* Include a row at the top of each file with variable names. 
* Make variable names human readable AgeAtDiagnosis instead of AgeDx
* In general data should be saved in one file per table.

[https://github.com/jtleek/datasharing](https://github.com/jtleek/datasharing)


---

## The code book

1. Information about the variables (including units!) in the data set not contained in the tidy data
2. Information about the summary choices you made
3. Information about the experimental study design you used


_Some other important tips_

* A common format for this document is a Word/text file. 
* There should be a section called "Study design" that has a thorough description of how you collected the data. 
* There must be a section called "Code book" that describes each variable and its units.


[https://github.com/jtleek/datasharing](https://github.com/jtleek/datasharing)


---

## The instruction list 

* Ideally a computer script (in R :-), but I suppose Python is ok too...)
* The input for the script is the raw data
* The output is the processed, tidy data
* There are no parameters to the script

In some cases it will not be possible to script every step. In that case you should provide instructions like: 

1. Step 1 - take the raw file, run version 3.1.2 of summarize software with parameters a=1, b=2, c=3
2. Step 2 - run the software separately for each sample
3. Step 3 - take column three of outputfile.out for each sample and that is the corresponding row in the output data set

[https://github.com/jtleek/datasharing](https://github.com/jtleek/datasharing)



---

## Why is the instruction list important? 

<img class=center src=../../assets/img/03_ObtainingData/rr.png height=200 />

<img class=center src=../../assets/img/03_ObtainingData/rrcolbert.jpeg height=250>


[http://www.colbertnation.com/the-colbert-report-videos/425748/april-23-2013/austerity-s-spreadsheet-error](http://www.colbertnation.com/the-colbert-report-videos/425748/april-23-2013/austerity-s-spreadsheet-error)

---

## Get/set your working directory

* A basic component of working with data is knowing your working directory
* The two main commands are ```getwd()``` and ```setwd()```. 
* Be aware of relative versus absolute paths
  * __Relative__ - ```setwd("./data")```, ```setwd("../")```
  * __Absolute__ - ```setwd("/Users/jtleek/data/")```
* Important difference in Windows ```setwd("C:\\Users\\Andrew\\Downloads")```

---

## Checking for and creating directories

* ```file.exists("directoryName")``` will check to see if the directory exists
* ```dir.create("directoryName")``` will create a directory if it doesn't exist
* Here is an example checking for a "data" directory and creating it if it doesn't exist

```{r mkdir}
if(!file.exists("data")){
  dir.create("data")
}
```


---

## Getting data from the internet - download.file()

* Downloads a file from the internet
* Even if you could do this by hand, helps with reproducibility
* Important parameters are _url_, _destfile_, _method_
* Useful for downloading tab-delimited, csv, and other files


---

## Example - Baltimore camera data


<img class=center src=../../assets/img/03_ObtainingData/cameras.png height=500>

[https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru](https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru)


---

## Example - Baltimore camera data

<img class=center src=../../assets/img/03_ObtainingData/cameraslink.png height=500>

[https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru](https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru)


---

## Download a file from the web

```{r data,dependson="mkdir"}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.csv",method="curl")
list.files("./data")
dateDownloaded <- date()
dateDownloaded
```

---

## Some notes about download.file()

* If the url starts with _http_ you can use download.file()
* If the url starts with _https_ on Windows you may be ok
* If the url starts with _https_ on Mac you may need to set _method="curl"_
* If the file is big, this might take a while
* Be sure to record when you downloaded. 

---

## Loading flat files - read.table()

* This is the main function for reading data into R
* Flexible and robust but requires more parameters
* Reads the data into RAM - big data can cause problems
* Important parameters _file_, _header_, _sep_, _row.names_, _nrows_
* Related: _read.csv()_, _read.csv2()_


---


## Example: Baltimore camera data

```{r}
cameraData <- read.table("./data/cameras.csv",sep=",",header=TRUE)
head(cameraData)
```

---

## Example: Baltimore camera data

read.csv sets _sep=","_ and _header=TRUE_ 
```{r,dependson="data"}
cameraData <- read.csv("./data/cameras.csv")
head(cameraData)
```


---

## Some more important parameters

* _quote_ - you can tell R whether there are any quoted values quote="" means no quotes.
* _na.strings_ - set the character that represents a missing value. 
* _nrows_ - how many rows to read of the file (e.g. nrows=10 reads 10 lines).
* _skip_ - number of lines to skip before starting to read

_In my experience, the biggest trouble with reading flat files are quotation marks ` or " placed in data values, setting quote="" often resolves these_.



---

## Example: Baltimore camera data

read.csv sets _sep=","_ and _header=TRUE_ 
```{r,dependson="data"}
cameraData <- scan("./data/cameras.csv",sep=",",what="character")
str(cameraData)
cameraData <- matrix(cameraData,byrow=TRUE,ncol=6)
```




---

## Excel files

_Still probably the most widely used format for sharing data_

<img class=center src=../../assets/img/03_ObtainingData/excel2.png height=450>


[http://office.microsoft.com/en-us/excel/](http://office.microsoft.com/en-us/excel/)

---

## Download the file to load

```{r edata}
if(!file.exists("data")){dir.create("data")}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.xlsx",method="curl")
dateDownloaded <- date()
```

---

## read.xlsx(), read.xlsx2() {xlsx package}

```{r xlsx,dependson="edata"}
library(xlsx)
cameraData <- read.xlsx("./data/cameras.xlsx",sheetIndex=1,header=TRUE)
head(cameraData)
```


---

## Reading specific rows and columns

```{r ,dependson="xlsx"}
colIndex <- 2:3
rowIndex <- 1:4
cameraDataSubset <- read.xlsx("./data/cameras.xlsx",sheetIndex=1,
                              colIndex=colIndex,rowIndex=rowIndex)
cameraDataSubset
```

---

## Further notes

* The _write.xlsx_ function will write out an Excel file with similar arguments.
* _read.xlsx2_ is much faster than _read.xlsx_ but for reading subsets of rows may be slightly unstable. 
* The [XLConnect](http://cran.r-project.org/web/packages/XLConnect/index.html) package has more options for writing and manipulating Excel files
* The [XLConnect vignette](http://cran.r-project.org/web/packages/XLConnect/vignettes/XLConnect.pdf) is a good place to start for that package
* In general it is advised to store your data in either a database
or in comma separated files (.csv) or tab separated files (.tab/.txt) as they are easier to distribute.

---


## XML

* Extensible markup language
* Frequently used to store structured data
* Particularly widely used in internet applications
* Extracting XML is the basis for most web scraping
* Components
  * Markup - labels that give the text structure
  * Content - the actual text of the document

[http://en.wikipedia.org/wiki/XML](http://en.wikipedia.org/wiki/XML)


---

## Tags, elements and attributes

* Tags correspond to general labels
  * Start tags `<section>`
  * End tags `</section>`
  * Empty tags `<line-break />`
* Elements are specific examples of tags
  * `<Greeting> Hello, world </Greeting>`
* Attributes are components of the label
  * `<img src="jeff.jpg" alt="instructor"/>`
  * `<step number="3"> Connect A to B. </step>`
  

[http://en.wikipedia.org/wiki/XML](http://en.wikipedia.org/wiki/XML)

---

## Example XML file

<img class=center src=../../assets/img/03_ObtainingData/xmlexample.png height=450>


[http://www.w3schools.com/xml/simple.xml](http://www.w3schools.com/xml/simple.xml)

---

## Read the file into R

```{r xmldata}
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl,useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
```


---

## Directly access parts of the XML document

```{r explore, dependson="xmldata"}
rootNode[[1]]
rootNode[[1]][[1]]
```



---

## Programatically extract parts of the file

```{r explore2, dependson="xmldata"}
xmlSApply(rootNode,xmlValue)
```

---

## Programatically extract parts of the file

```{r explore3, dependson="xmldata"}
xmlSApply(rootNode,xmlValue)
```


---

## XPath

* _/node_ Top level node
* _//node_ Node at any level
* _node[@attr-name]_ Node with an attribute name
* _node[@attr-name='bob']_ Node with attribute name attr-name='bob'

Information from: [http://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf](http://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf)


---

## Get the items on the menu and prices

```{r explore4, dependson="xmldata"}
xpathSApply(rootNode,"//name",xmlValue)
xpathSApply(rootNode,"//price",xmlValue)
```



---

## Another example


<img class=center src=../../assets/img/03_ObtainingData/ravens.png height=450>

[http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens](http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens)


---

## Viewing the source

<img class=center src=../../assets/img/03_ObtainingData/ravenssource.png height=450>

[http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens](http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens)


---

## Extract content by attributes

```{r htmldata}
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE)
scores <- xpathSApply(doc,"//li[@class='score']",xmlValue)
teams <- xpathSApply(doc,"//li[@class='team-name']",xmlValue)
scores
teams
```

---

## Notes and further resources

* Official XML tutorials [short](http://www.omegahat.org/RSXML/shortIntro.pdf), [long](http://www.omegahat.org/RSXML/Tour.pdf)
* [An outstanding guide to the XML package](http://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf)

---

## JSON

* Javascript Object Notation
* Lightweight data storage
* Common format for data from application programming interfaces (APIs)
* Similar structure to XML but different syntax/format
* Data stored as
  * Numbers (double)
  * Strings (double quoted)
  * Boolean (_true_ or _false_)
  * Array (ordered, comma separated enclosed in square brackets _[]_)
  * Object (unorderd, comma separated collection of key:value pairs in curley brackets _{}_)


[http://en.wikipedia.org/wiki/JSON](http://en.wikipedia.org/wiki/JSON)


---

## Example JSON file

<img class=center src=../../assets/img/03_ObtainingData/githubjson.png height=450>


---

## Reading data from JSON {jsonlite package}

```{r readJSON}
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
names(jsonData)
jsonData$name
```


---

## Nested objects in JSON

```{r,dependson="readJSON"}
names(jsonData$owner)
jsonData$owner$login
```


---

## Writing data frames to JSON

```{r writeJSON}
myjson <- toJSON(iris, pretty=TRUE)
cat(myjson)
```

[http://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder/](http://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder/)


---

## Convert back to JSON

```{r,dependson="writeJSON"}
iris2 <- fromJSON(myjson)
head(iris2)
```


[http://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder/](http://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder/)

---

## Further resources

* [http://www.json.org/](http://www.json.org/)
* A good tutorial on jsonlite - [http://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder/](http://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder/)
* [jsonlite vignette](http://cran.r-project.org/web/packages/jsonlite/vignettes/json-mapping.pdf)

---

## mySQL

* Free and widely used open source database software
* Widely used in internet based applications
* Data are structured in 
  * Databases
  * Tables within databases
  * Fields within tables
* Each row is called a record

[http://en.wikipedia.org/wiki/MySQL](http://en.wikipedia.org/wiki/MySQL)
[http://www.mysql.com/](http://www.mysql.com/)


---

## Example structure

<img class=center src=../../assets/img/03_ObtainingData/database-schema.png height=450>


[http://dev.mysql.com/doc/employee/en/sakila-structure.html](http://dev.mysql.com/doc/employee/en/sakila-structure.html)

---

## Step 1 - Install MySQL

<img class=center src=../../assets/img/03_ObtainingData/installmysql.png height=450>

[http://dev.mysql.com/doc/refman/5.7/en/installing.html](http://dev.mysql.com/doc/refman/5.7/en/installing.html)

---

## Step 2 - Install RMySQL

* On a Mac: ```install.packages("RMySQL")```
* On Windows: 
  * Official instructions - [http://biostat.mc.vanderbilt.edu/wiki/Main/RMySQL](http://biostat.mc.vanderbilt.edu/wiki/Main/RMySQL) (may be useful for Mac/UNIX users as well)
  * Potentially useful guide - [http://www.ahschulz.de/2013/07/23/installing-rmysql-under-windows/](http://www.ahschulz.de/2013/07/23/installing-rmysql-under-windows/)  


---

## Example - UCSC database


<img class=center src=../../assets/img/03_ObtainingData/ucsc.png height=450>

[http://genome.ucsc.edu/](http://genome.ucsc.edu/)



---

## UCSC MySQL


<img class=center src=../../assets/img/03_ObtainingData/ucscmysql.png height=450>

[http://genome.ucsc.edu/goldenPath/help/mysql.html](http://genome.ucsc.edu/goldenPath/help/mysql.html)


---

## Connecting and listing databases

```{r databases}
library(RMySQL)
ucscDb <- dbConnect(MySQL(),user="genome", 
                    host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb,"show databases;"); dbDisconnect(ucscDb);
result
```


---

## Connecting to hg19 and listing tables

```{r tables}
hg19 <- dbConnect(MySQL(),user="genome", db="hg19",
                    host="genome-mysql.cse.ucsc.edu")
allTables <- dbListTables(hg19)
length(allTables)
allTables[1:5]
```


---

## Get dimensions of a specific table

```{r dimensions,dependson="tables"}
dbListFields(hg19,"affyU133Plus2")
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
```


---

## Read from the table

```{r readdata ,dependson="tables"}
affyData <- dbReadTable(hg19, "affyU133Plus2")
head(affyData)
```


---

## Select a specific subset

```{r, dependson="tables"}
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query); quantile(affyMis$misMatches)
affyMisSmall <- fetch(query,n=10); dbClearResult(query);
dim(affyMisSmall)
```

---

## Don't forget to close the connection!

```{r, dependson="tables"}
dbDisconnect(hg19)
```

---

## Further resources

* RMySQL vignette [http://cran.r-project.org/web/packages/RMySQL/RMySQL.pdf](http://cran.r-project.org/web/packages/RMySQL/RMySQL.pdf)
* List of commands [http://www.pantz.org/software/mysql/mysqlcommands.html](http://www.pantz.org/software/mysql/mysqlcommands.html)
  * __Do not, do not, delete, add or join things from ensembl. Only select.__
  * In general be careful with mysql commands
* A nice blog post summarizing some other commands [http://www.r-bloggers.com/mysql-and-r/](http://www.r-bloggers.com/mysql-and-r/)


---

## Application programming interfaces


<img class=center src=../../assets/img/03_ObtainingData/twitter.png height= 450/>


[https://dev.twitter.com/docs/api/1/get/blocks/blocking](https://dev.twitter.com/docs/api/1/get/blocks/blocking)


---

## Creating an application

<img class=center src=../../assets/img/03_ObtainingData/twitterapp1.png height= 450/>

[https://dev.twitter.com/apps](https://dev.twitter.com/appsmyapp <- oauth_app("twitter", key = "TYrWFPkFAkn4G5BbkWINYw"))

---

## Creating an application

<img class=center src=../../assets/img/03_ObtainingData/twitterapp2.png height= 450/>


---

## Creating an application

<img class=center src=../../assets/img/03_ObtainingData/twitterapp3.png height= 450/>


---

## Accessing Twitter from R

```{r,eval=FALSE}
myapp = oauth_app("twitter",
                   key="yourConsumerKeyHere",secret="yourConsumerSecretHere")
sig = sign_oauth1.0(myapp,
                     token = "yourTokenHere",
                      token_secret = "yourTokenSecretHere")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
```


---

## Converting the json object

```{r eval=FALSE}
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1,1:4]
```

```
                      created_at           id             id_str
1 Mon Jan 13 05:18:04 +0000 2014 4.225984e+17 422598398940684288
                                                                                                                                         text
1 Now that P. Norvig's regex golf IPython notebook hit Slashdot, let's see if our traffic spike tops the previous one: http://t.co/Vc6JhZXOo8
```


---

## How did I know what url to use?


<img class=center src=../../assets/img/03_ObtainingData/twitterurl.png height= 450/>

[https://dev.twitter.com/docs/api/1.1/get/search/tweets](https://dev.twitter.com/docs/api/1.1/get/search/tweets)

---

## In general look at the documentation


<img class=center src=../../assets/img/03_ObtainingData/twitterdocs.png height= 450/>

[https://dev.twitter.com/docs/api/1.1/overview](https://dev.twitter.com/docs/api/1.1/overview)



---

## In general look at the documentation


* httr allows `GET`, `POST`, `PUT`, `DELETE` requests if you are authorized
* You can authenticate with a user name or a password
* Most modern APIs use something like oauth
* httr works well with Facebook, Google, Twitter, Githb, etc. 

---

## data.table

* Inherets from data.frame
  * All functions that accept data.frame work on data.table
* Written in C so it is much faster
* Much, much faster at subsetting, group, and updating
 

---

## Create data tables just like data frames

```{r init}
library(data.table)
DF = data.frame(x=rnorm(9),y=rep(c("a","b","c"),each=3),z=rnorm(9))
head(DF,3)
DT = data.table(x=rnorm(9),y=rep(c("a","b","c"),each=3),z=rnorm(9))
head(DT,3)
```

---

## See all the data tables in memory

```{r,dependson="init"}
tables()
```

---

## Subsetting rows

```{r,dependson="init"}
DT[2,]
DT[DT$y=="a",]
```

---

## Subsetting rows

```{r,dependson="init"}
DT[c(2,3)]
```


---

## Subsetting columns!?

```{r,dependson="init"}
DT[,c(2,3)]
```

---

## Column subsetting in data.table

* The subsetting function is modified for data.table
* The argument you pass after the comma is called an "expression"
* In R an expression is a collection of statements enclosed in curley brackets 
```{r }
{
  x = 1
  y = 2
}
k = {print(10); 5}
print(k)
```

---

## Calculating values for variables with expressions

```{r problemchunk,dependson="init"}
DT = data.table(x=rnorm(9),y=rep(c("a","b","c"),each=3),z=rnorm(9))
DT[,list(mean(x),sum(z))]
DT[,table(y)]
```

---

## Adding new columns

```{r,dependson="init"}
DT[,w:=z^2]
```


---

## Adding new columns

```{r dt2,dependson="init"}
DT2 <- DT
DT[, y:= 2]
```

---

## Careful

```{r ,dependson="dt2"}
head(DT,n=3)
head(DT2,n=3)
```


---

## Multiple operations

```{r,dependson="init"}
DT[,m:= {tmp <- (x+z); log2(tmp+5)}]
```

---

## plyr like operations

```{r,dependson="init"}
DT[,a:=x>0]
```


---

## plyr like operations

```{r,dependson="init"}
DT[,b:= mean(x+w),by=a]
```


---

## Special variables

`.N` An integer, length 1, containing the numbe r

```{r}
set.seed(123);
DT <- data.table(x=sample(letters[1:3], 1E5, TRUE))
DT[, .N, by=x]
```

---

## Keys

```{r}
DT <- data.table(x=rep(c("a","b","c"),each=100), y=rnorm(300))
setkey(DT, x)
DT['a']
```

---

## Joins

```{r}
DT1 <- data.table(x=c('a', 'a', 'b', 'dt1'), y=1:4)
DT2 <- data.table(x=c('a', 'b', 'dt2'), z=5:7)
setkey(DT1, x); setkey(DT2, x)
merge(DT1, DT2)
```



---

## Fast reading

```{r,cache=TRUE}
big_df <- data.frame(x=rnorm(1E6), y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names=FALSE, col.names=TRUE, sep="\t", quote=FALSE)
system.time(fread(file))
system.time(read.table(file, header=TRUE, sep="\t"))
```




---

## Summary and further reading

* The latest development version contains new functions like `melt` and `dcast` for data.tables 
  * [https://r-forge.r-project.org/scm/viewvc.php/pkg/NEWS?view=markup&root=datatable](https://r-forge.r-project.org/scm/viewvc.php/pkg/NEWS?view=markup&root=datatable)
* Here is a list of differences between data.table and data.frame
  * [http://stackoverflow.com/questions/13618488/what-you-can-do-with-data-frame-that-you-cant-in-data-table](http://stackoverflow.com/questions/13618488/what-you-can-do-with-data-frame-that-you-cant-in-data-table)
* Notes based on Raphael Gottardo's notes [https://github.com/raphg/Biostat-578/blob/master/Advanced_data_manipulation.Rpres](https://github.com/raphg/Biostat-578/blob/master/Advanced_data_manipulation.Rpres), who got them from Kevin Ushey.
  